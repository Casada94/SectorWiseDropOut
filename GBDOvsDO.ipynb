{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize"
   ],
   "metadata": {
    "id": "_6HyxWPcm7xX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698212419142,
     "user_tz": 420,
     "elapsed": 192,
     "user": {
      "displayName": "Clayton A.",
      "userId": "01902548545842697740"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-10-31T04:06:40.566178399Z",
     "start_time": "2023-10-31T04:06:38.455622990Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 21:06:39.191422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def standardize(arr):\n",
    "    arr[0] = (abs(arr[0]) - 3.870671003)/1.899821718\n",
    "    arr[1] = (abs(arr[1]) - 28.63948643)/12.58555761\n",
    "    arr[2] = (abs(arr[2]) - 5.428999742)/2.474173139\n",
    "    arr[3] = (abs(arr[3]) - 1.09667515)/0.473910857\n",
    "    arr[4] = (abs(arr[4]) - 1425.476744)/1132.462122\n",
    "    arr[5] = (abs(arr[5]) - 3.070655159)/10.38604956\n",
    "    arr[6] = (abs(arr[6]) - 35.63186143)/2.135952397\n",
    "    arr[7] = (abs(arr[7]) - 119.5697045)/2.003531724\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T06:06:18.087809460Z",
     "start_time": "2023-10-31T06:06:18.046318598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#import dataset\n",
    "raw = fetch_california_housing(as_frame=True)\n",
    "\n",
    "#get input and output\n",
    "X = raw.data.values\n",
    "np.apply_along_axis(standardize,axis=1,arr=X)\n",
    "Y = raw.target.values\n",
    "n = len(X)\n",
    "r = 0.25\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle=True, test_size=r)\n",
    "\n",
    "x_val = x_train[:int(n*r)]\n",
    "part_x_train = x_train[int(n*r):]\n",
    "y_val = y_train[:int(n*r)]\n",
    "part_y_train = y_train[int(n*r):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T06:06:36.497793465Z",
     "start_time": "2023-10-31T06:06:36.429854402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.8008 - val_loss: 0.5502\n",
      "Epoch 2/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.6028 - val_loss: 0.5067\n",
      "Epoch 3/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.5660 - val_loss: 0.4684\n",
      "Epoch 4/200\n",
      "10320/10320 [==============================] - 20s 2ms/step - loss: 0.5388 - val_loss: 0.4655\n",
      "Epoch 5/200\n",
      "10320/10320 [==============================] - 21s 2ms/step - loss: 0.5265 - val_loss: 0.4382\n",
      "Epoch 6/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.5183 - val_loss: 0.4457\n",
      "Epoch 7/200\n",
      "10320/10320 [==============================] - 25s 2ms/step - loss: 0.5097 - val_loss: 0.4568\n",
      "Epoch 8/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.5027 - val_loss: 0.4243\n",
      "Epoch 9/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4954 - val_loss: 0.4280\n",
      "Epoch 10/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4857 - val_loss: 0.4446\n",
      "Epoch 11/200\n",
      "10320/10320 [==============================] - 19s 2ms/step - loss: 0.4797 - val_loss: 0.4186\n",
      "Epoch 12/200\n",
      "10320/10320 [==============================] - 20s 2ms/step - loss: 0.4766 - val_loss: 0.4257\n",
      "Epoch 13/200\n",
      "10320/10320 [==============================] - 20s 2ms/step - loss: 0.4847 - val_loss: 0.4040\n",
      "Epoch 14/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4813 - val_loss: 0.4140\n",
      "Epoch 15/200\n",
      "10320/10320 [==============================] - 20s 2ms/step - loss: 0.4692 - val_loss: 0.3979\n",
      "Epoch 16/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4650 - val_loss: 0.4176\n",
      "Epoch 17/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4618 - val_loss: 0.4013\n",
      "Epoch 18/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4581 - val_loss: 0.4043\n",
      "Epoch 19/200\n",
      "10320/10320 [==============================] - 28s 3ms/step - loss: 0.4617 - val_loss: 0.3922\n",
      "Epoch 20/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4594 - val_loss: 0.3954\n",
      "Epoch 21/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4610 - val_loss: 0.4060\n",
      "Epoch 22/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4525 - val_loss: 0.3841\n",
      "Epoch 23/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4469 - val_loss: 0.4094\n",
      "Epoch 24/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4463 - val_loss: 0.3888\n",
      "Epoch 25/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4481 - val_loss: 0.3827\n",
      "Epoch 26/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4427 - val_loss: 0.4068\n",
      "Epoch 27/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4430 - val_loss: 0.3764\n",
      "Epoch 28/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4478 - val_loss: 0.3856\n",
      "Epoch 29/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4452 - val_loss: 0.3845\n",
      "Epoch 30/200\n",
      "10320/10320 [==============================] - 19s 2ms/step - loss: 0.4395 - val_loss: 0.3799\n",
      "Epoch 31/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4387 - val_loss: 0.3722\n",
      "Epoch 32/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4414 - val_loss: 0.3738\n",
      "Epoch 33/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4333 - val_loss: 0.3791\n",
      "Epoch 34/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4300 - val_loss: 0.3797\n",
      "Epoch 35/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4302 - val_loss: 0.3665\n",
      "Epoch 36/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4289 - val_loss: 0.3639\n",
      "Epoch 37/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4275 - val_loss: 0.3656\n",
      "Epoch 38/200\n",
      "10320/10320 [==============================] - 20s 2ms/step - loss: 0.4314 - val_loss: 0.3727\n",
      "Epoch 39/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4281 - val_loss: 0.3729\n",
      "Epoch 40/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4294 - val_loss: 0.3747\n",
      "Epoch 41/200\n",
      "10320/10320 [==============================] - 21s 2ms/step - loss: 0.4251 - val_loss: 0.3838\n",
      "Epoch 42/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4207 - val_loss: 0.3623\n",
      "Epoch 43/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4241 - val_loss: 0.3584\n",
      "Epoch 44/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4202 - val_loss: 0.3579\n",
      "Epoch 45/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4207 - val_loss: 0.3622\n",
      "Epoch 46/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4209 - val_loss: 0.3580\n",
      "Epoch 47/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4203 - val_loss: 0.3578\n",
      "Epoch 48/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4221 - val_loss: 0.3607\n",
      "Epoch 49/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4214 - val_loss: 0.3837\n",
      "Epoch 50/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4170 - val_loss: 0.3597\n",
      "Epoch 51/200\n",
      "10320/10320 [==============================] - 23s 2ms/step - loss: 0.4148 - val_loss: 0.3550\n",
      "Epoch 52/200\n",
      "10320/10320 [==============================] - 26s 2ms/step - loss: 0.4174 - val_loss: 0.3774\n",
      "Epoch 53/200\n",
      "10320/10320 [==============================] - 22s 2ms/step - loss: 0.4134 - val_loss: 0.3718\n",
      "Epoch 54/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4198 - val_loss: 0.3573\n",
      "Epoch 55/200\n",
      "10320/10320 [==============================] - 26s 2ms/step - loss: 0.4170 - val_loss: 0.3571\n",
      "Epoch 56/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4128 - val_loss: 0.3496\n",
      "Epoch 57/200\n",
      "10320/10320 [==============================] - 25s 2ms/step - loss: 0.4137 - val_loss: 0.3511\n",
      "Epoch 58/200\n",
      "10320/10320 [==============================] - 25s 2ms/step - loss: 0.4091 - val_loss: 0.3661\n",
      "Epoch 59/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4091 - val_loss: 0.3553\n",
      "Epoch 60/200\n",
      "10320/10320 [==============================] - 24s 2ms/step - loss: 0.4096 - val_loss: 0.3682\n",
      "Epoch 61/200\n",
      "10320/10320 [==============================] - 21s 2ms/step - loss: 0.4103 - val_loss: 0.3524\n",
      "Epoch 62/200\n",
      "10320/10320 [==============================] - 28s 3ms/step - loss: 0.4127 - val_loss: 0.3553\n",
      "Epoch 63/200\n",
      "  671/10320 [>.............................] - ETA: 21s - loss: 0.3950"
     ]
    }
   ],
   "source": [
    "from MultiLevelLayer import MultiLevel\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(8,)))\n",
    "model.add(tf.keras.layers.Dropout(0))\n",
    "model.add(tf.keras.layers.Dense(32,activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dropout(.20))\n",
    "# model.add(MultiLevel(2,False))\n",
    "model.add(tf.keras.layers.Dense(16,activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dropout(.20))\n",
    "model.add(tf.keras.layers.Dense(1,activation='relu'))\n",
    "tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "\n",
    "historyBefore = model.fit(part_x_train,\n",
    "                    part_y_train,validation_data=(x_val,y_val),\n",
    "                    epochs=200,\n",
    "                    batch_size=1\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-31T06:09:33.392500760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "thing = model.predict(x_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYpSBrC2jYtg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698215876087,
     "user_tz": 420,
     "elapsed": 177,
     "user": {
      "displayName": "Clayton A.",
      "userId": "01902548545842697740"
     }
    },
    "outputId": "d03a996e-8d29-400d-a626-2ff3c5d4609f",
    "ExecuteTime": {
     "start_time": "2023-10-31T03:33:38.879649126Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.scatter(thing,y_test,s=2)\n",
    "plt.title('Median House Value')\n",
    "plt.xlabel('Predicted Value (100 thousand Dollars)')\n",
    "plt.ylabel('Actual Value (100 thousand Dollars)')\n",
    "x_temp=[-.25,5.5]\n",
    "y_temp=[-0.25,5.5]\n",
    "plt.plot(x_temp, y_temp, 'r')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Rk7y9Tu2jWy3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1698215920232,
     "user_tz": 420,
     "elapsed": 407,
     "user": {
      "displayName": "Clayton A.",
      "userId": "01902548545842697740"
     }
    },
    "outputId": "3ccf454a-74e3-45dd-f61b-730a7f3bc7b6",
    "ExecuteTime": {
     "start_time": "2023-10-31T03:33:38.879726084Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scalar =2\n",
    "use_all_nodes = False\n",
    "level_size=2\n",
    "\n",
    "arr = np.full(32, 0, dtype='float32')\n",
    "for i in range(0, len(arr), level_size):\n",
    "    temp = tf.random.uniform(shape=(), minval=0, maxval=level_size,dtype='int32')\n",
    "    arr[i + temp] = scalar if not use_all_nodes else 1\n",
    "\n",
    "print(arr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T03:33:38.880524650Z",
     "start_time": "2023-10-31T03:33:38.879759888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SliceMasker:\n",
    "    def __init__(self, num_samples, num_features, step_size):\n",
    "        self.pseudo_logits = tf.ones(shape=(1, step_size))\n",
    "        self.num_masked = num_samples//step_size\n",
    "        self.offsets = tf.expand_dims(\n",
    "            tf.range(self.num_masked) * step_size, axis=0)\n",
    "        self.mask = tf.zeros((self.num_masked, num_features))\n",
    "\n",
    "    def mask_slices(self, tensor):\n",
    "        slice_ids = tf.random.categorical(\n",
    "            self.pseudo_logits, self.num_masked, dtype=tf.int32)\n",
    "        slice_ids = tf.reshape(slice_ids + self.offsets, (self.num_masked, 1))\n",
    "        return tf.tensor_scatter_nd_update(tensor, slice_ids, self.mask)\n",
    "\n",
    "\n",
    "num_samples = 21\n",
    "num_features = 1\n",
    "step_size = 3\n",
    "\n",
    "sm = SliceMasker(num_samples, num_features, step_size)\n",
    "\n",
    "\n",
    "example = tf.random.normal(shape=(num_samples, num_features))\n",
    "\n",
    "print(\"before masking:\")\n",
    "print(example.numpy())\n",
    "masked = sm.mask_slices(example)\n",
    "print(\"after masking:\")\n",
    "print(masked.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-31T03:33:38.879784930Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
